---
title: "CueStrength within subject"
author: "Manuel Bohn"
date: "17 3 2018"
output: html_document
---

```{r setup}
###
library(tidyverse)
library(knitr)
library(langcog)
library(ggthemes)
library(ggthemes)


## importing data
d <- read_csv(file="cue_within.data.csv") %>%
  mutate(trial_type = ifelse(trial == "train", "train", "test"))

```

Sanity checks.
```{r training performance}
## performance in training and test
d %>%
  group_by(trial_type) %>%
  summarise(n = length(unique(id)), 
            correct = mean(correct)) %>%
  knitr::kable(digits = 3)

```

Plot with all data for all conditions. Interestingly, the type of communicative act does not seem to matter. I wonder if the same would be true for children.

This is all within subject, so there might be carry over effects. Next I'm only looking at trial 1.
```{r plot overall}

ms <- d %>%
  filter(trial_type == "test") %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  multi_boot_standard(col = "correct")

ggplot(ms, 
       aes(x = condition, y = mean, fill = condition)) +
  geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_hline(yintercept = 0.5, lty=2)+
  labs(x="",y="Proportion Expected Choice")+
  theme_few(base_size = 12) + 
  ylim(0,1)+
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())+
  scale_fill_solarized(name="Condition",
                     breaks=c("look", "lookLabel","point","pointLabel"),
                     labels=c("Look", "Look + Label","Point","Point + Label"))
```

First, check how much data for trial 1 we have per condition (order was completely randomized). Roughly equally distributed.
```{r trial 1 data}
d %>%
  filter(trial == "1") %>%
  group_by(condition) %>%
  summarise(n = length(unique(id)), 
            correct = mean(correct)) %>%
  knitr::kable(digits = 2)
```

Plot for trial 1. Similar to overall pattern, maybe a little weaker for look alone. Not sure if following up between subject would be worth it? 
```{r plot trial 1}
ms <- d %>%
  filter(trial == "1") %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  multi_boot_standard(col = "correct")

ggplot(ms, 
       aes(x = condition, y = mean, fill = condition)) +
  geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_hline(yintercept = 0.5, lty=2)+
  labs(x="",y="Proportion Expected Choice")+
  theme_few(base_size = 12) + 
  ylim(0,1)+
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())+
  scale_fill_solarized(name="Condition",
                     breaks=c("look", "lookLabel","point","pointLabel"),
                     labels=c("Look", "Look + Label","Point","Point + Label"))
```

Effect of target object location on table. Inner = closer to the agent, outer = further away. 
General bias to pick the inner object, considerably stronger for point. Probably because the poitning finger is closer to the inner object.  
```{r inner vs outer location}
## performance in training and test
ms <- d %>%
  filter(trial_type == "test") %>%
  group_by(targetOnTable,condition, id) %>%
  summarise(correct = mean(correct)) %>%
  multi_boot_standard(col = "correct")

ggplot(ms, 
       aes(x = condition, y = mean, fill = condition,frame = targetOnTable)) +
  geom_bar(stat="identity") + 
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_hline(yintercept = 0.5, lty=2)+
  facet_wrap(~ targetOnTable) +
  labs(x="",y="Proportion Expected Choice")+
  theme_few(base_size = 12) +
  ylim(0,1)+
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())+
  scale_fill_solarized(name="Condition",
                     breaks=c("look", "lookLabel","point","pointLabel"),
                     labels=c("Look", "Look + Label","Point","Point + Label"))

```

Condition by trial overall. Looks pretty random.
```{r plot trials}
ms <- d %>%
  filter(trial_type == "test") %>%
  group_by(condition, trial, id) %>%
  summarise(correct = mean(correct)) %>%
  multi_boot_standard(col = "correct")

ggplot(ms, 
       aes(x = trial, y = mean, col = condition)) +
  geom_line(aes(group= condition)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1)) + 
  geom_hline(yintercept = 0.5, lty=2)+
  theme_few() + 
  scale_colour_solarized()
```

Conditions by agents.
```{r trials per condition and agent}
# d %>%
#   filter(trial_type == "test") %>%
#   group_by(agent,condition) %>%
#   summarise(n = length(agent), 
#             correct = mean(correct)) %>%
#   knitr::kable(digits = 2)
```

Agent effect. Cat seems a little strange in point label, but also very few data.
```{r plot agents}
ms <- d %>%
  filter(trial_type == "test") %>%
  group_by(condition, agent, id) %>%
  summarise(correct = mean(correct)) %>%
  multi_boot_standard(col = "correct")

ggplot(ms, 
       aes(x = agent, y = mean, col = condition)) +
  geom_line(aes(group= condition)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1)) + 
  geom_hline(yintercept = 0.5, lty=2)+
  theme_few() + 
  scale_colour_solarized()
```

Tests against chance per condition. We only have 2 trials per subject per condition, that's why t-tests aren't the best option. I used wilcoxon tests instead. We would get the same info out of the model but this might be helpful for a quick overview. There is no Bayesian version of a wilcoxon test, yet.
```{r tests against chance}
## Wilcoxon tests against chance
library(exactRankTests)

d %>%
  filter(trial_type == "test") %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(mean= mean(unlist(correct)),
         stat = wilcox.exact(unlist(correct), mu = 0.5)$statistic,
         p = wilcox.exact(unlist(correct), mu = 0.5)$p.value) %>%
  select(condition,mean,stat,p) %>%
  knitr::kable(digits = 3)
 
```

Brm Model.

```{r model}
## model
library(brms)

data_inf <- d%>%
  filter(trial_type == "test")


prior <- get_prior(correct ~ condition +
                      (condition |id) + (condition |agent),
                    data = data_inf, family = bernoulli()) 

# registered  model
bm <- brm(correct ~ condition +
              (condition |id) + (condition |agent), 
          data = data_inf, family = bernoulli(),
          control = list(adapt_delta = 0.90),
          sample_prior = T,
          prior = prior,
          iter = 2000)

summary(bm)


# model with default priors

#Population-Level Effects: 
#                    Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
#Intercept               1.86      0.36     1.22     2.62       2190 1.00
#conditionlookLabel      0.04      0.50    -0.88     1.09       1851 1.00
#conditionpoint          0.51      1.24    -0.95     3.86        160 1.03
#conditionpointLabel     0.21      0.65    -0.84     1.75       1194 1.00

# model with priors from get priors

#Population-Level Effects: 
#                    Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
#Intercept               1.85      0.36     1.17     2.61       2811 1.00
#conditionlookLabel      0.03      0.50    -0.87     1.12       1886 1.00
#conditionpoint          0.28      0.83    -0.93     2.35        928 1.00
#conditionpointLabel     0.22      0.66    -0.86     1.81       1313 1.00


h <- c("conditionlookLabel - conditionpoint > 0 ","conditionpointLabel - conditionlookLabel > 0","conditionpointLabel - conditionpoint > 0")

a <- hypothesis(bm, h,alpha = 0.05)

p <- plot(a, plot = F, theme = theme_get())[[1]]

p +
  xlim(-5,5)+
  theme_few()
```

```{r}
# 20 000 iterations and adapt_delta = 0.99
# 
# Population-Level Effects: 
#                     Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
# Intercept               1.85      0.36     1.20     2.62      40000 1.00
# conditionlookLabel      0.03      0.49    -0.87     1.09      40000 1.00
# conditionpoint          0.44      1.27    -0.94     3.62       3751 1.00
# conditionpointLabel     0.23      0.67    -0.86     1.80      16832 1.00
```



```{r bayes factor, include = FALSE}


full <- brm(correct ~ condition +
              (condition |id) + (condition |agent), 
          data = data_inf, family = bernoulli(),
          control = list(adapt_delta = 0.90),
          save_all_pars = TRUE,
          sample_prior = F,
          prior = prior,
          iter = 2000)

# null model

prior2 <- get_prior(correct ~ 1 +
                      (condition |id) + (condition |agent),
                    data = data_inf, family = bernoulli()) 

null <- brm(correct ~ 1 +
               (condition |id) + (condition |agent), 
           data = data_inf, family = bernoulli(),
           control = list(adapt_delta = 0.90),
           save_all_pars = TRUE,
           sample_prior = F,
           prior = prior2,
           iter = 2000)
 

 bf <- bayes_factor(full, null, log = FALSE, maxiter = 1000)




```

